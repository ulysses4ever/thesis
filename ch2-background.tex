% ****************************************************************************************
%              Chap. 1: Intro
% ****************************************************************************************

\chapter{Background and Motivation}
\label{chap-back}

\section{Julia Background}%
\label{sec-julia}

The Julia language is designed around multiple dispatch~\cite{BezansonEKS17}.
Programs consist of \emph{functions} that are implemented by multiple
\emph{methods} of the same name; each method is distinguished by a distinct type
signature, and all methods are stored in a so-called method table.
At run time, the Julia implementation dispatches a function call to
the \emph{most specific} method by comparing the types of the arguments to the
types of the parameters of all methods of that function. As an example of a
function and its constituent methods, consider the \c{+} function. As of version 1.5.4 of the
language, there are 190 implementations of \c{+}, each covering a
specific case determined by its type signature. Fig.~\ref{plus} displays custom
implementations for 16-bit floating point numbers, missing values,
big-floats/big-integers, and complex arithmetic.
Although at the source-code level, multiple methods look similar to
overloading known from languages like C++ and Java, the key difference is that
those languages resolve overloading statically whereas Julia does that
dynamically using multiple dispatch.

\begin{figure}
\begin{lstlisting}[language=julia]
# 184 methods for generic function "+":
[1] +(a::Float16, b::Float16) in Base at float.jl:398
[2] +(::Missing, ::Missing) in Base at missing.jl:114
[3] +(::Missing) in Base at missing.jl:100
[4] +(::Missing, ::Number) in Base at missing.jl:115
[5] +(a::BigFloat, b::BigFloat, c::BigFloat, d::BigFloat) in Base.MPFR at mpfr.jl:541
[6] +(a::BigFloat, b::BigFloat, c::BigFloat) in Base.MPFR at mpfr.jl:535
[7] +(x::BigFloat, c::BigInt) in Base.MPFR at mpfr.jl:394
[8] +(x::BigFloat, y::BigFloat) in Base.MPFR at mpfr.jl:363
...
\end{lstlisting}
\caption{Methods from the standard library}\label{plus}
\end{figure}

Julia supports a rich type language for defining method
signatures. Base types consist of either bits types---types that have a direct
binary representation, like integers---or structs. Both bits types and struct
types, referred to as \emph{concrete types}, can have supertypes, but all
supertypes are \emph{abstract types}. Abstract types can be arranged into a
single-subtyping hierarchy rooted at \c{Any}, and no abstract type can be
instantiated. The type language allows for further composition of these base
types using unions, tuples, and bounded existential constructors; the result of
composition can be abstract or concrete. \citet{oopsla18b} gives a detailed
discussion of the type language and of subtyping.

Any function call in a program, such as \c{x+y}, requires choosing one of the
methods of the target function. \emph{Method dispatch} chooses the method using
a multi-step process.
First, the implementation obtains the concrete types of arguments. Second, it
retrieves applicable methods by checking for subtyping between argument types
and type annotations of the methods. Next, it sorts these methods into subtype
order. Finally, the call is dispatched to the most specific method---a method
such that no other applicable method is its strict subtype. If no such
method exists, an error is produced. As an example, consider the above
definition of \c{+}: a call with two \c{BigFloat}'s dispatches to
definition 8 from \figref{plus}: \c{+(x::BigFloat, y::BigFloat)}.

Function calls are pervasive in Julia, and their efficiency is crucial for
performance. However, the many complex type-level operations involved in dispatch
make the process slow. Moreover, the language implementation, as of this writing,
does not perform inline caching~\cite{DS84}, meaning that dispatch results are
not cached across calls. To attain acceptable performance, the compiler attempts
to remove as many dispatch operations as it can. This optimization leverages
run-time type information whenever a method is compiled, i.e., when it is called
for the first time with a novel set of argument types.  These types are used by
the compiler to infer types in the method body. Then, this type information
frequently allows the compiler to devirtualize and inline the function calls
within a method~\cite{aigner}, thus improving performance. However, this
optimization is not always possible: if type inference cannot produce a
sufficiently specific type, then the call cannot be devirtualized. Consider the
prior example of \c{x+y}: the method to call cannot be determined
if \c{y} is known to be one of \c{BigFloat} or \c{BigInt}. This problem arises for
various reasons, for example, accessing a struct field of an abstract type, or
the type inferencer losing precision due to a branching statement. A more
detailed description of the compilation strategy and its performance is given in
\cite{oopsla18a}.

\section{Type Stability: a Key to Performance?}%
\label{sec:stability}

Removing dispatch is the key to performance, but to perform the optimization,
the compiler needs precise type information. Thus, while developers
are encouraged to write generic code, the code also needs to be conducive
to type inference and type-based optimizations.
In this section, I give an overview of the appropriate coding discipline, and
explain how it enables optimizations.

\subsection{Julia Performance}\label{ssect:perf}

To illustrate performance implications of careless
coding practices, consider Fig.~\ref{ide}, which displays a method for one of
the Julia microbenchmarks, \c{pisum}. For the purposes of this example, we
have added an identity function \c{id} which was initially implemented to
return its argument in both branches, as well-behaved identities do.
Then, the \c{id} method was changed to return a string in the impossible
branch (\c{rand()} returns a value from 0 to 1). The impact of that change
was about a 40\% increase in the execution time of the benchmark (\juliaversion).

\input{figs/benchmark}

When a performance regression occurs, it is common for developers to study the
intermediate representation produced by the compiler. To facilitate this, the
language provides a macro, \c{code_warntype}, that shows the code along with the
inferred types for a given function invocation. Fig.~\ref{ide} demonstrates the result of
calling \c{code_warntype} on \c{id(5)}. Types that are imprecise, i.e., not
concrete, show up in red: they indicate that concrete type of a value may vary
from run to run. Here, we see that when called with an integer,
\c{id} may return either an
\c{Int64} or a \c{String}.
Moreover, the imprecise return type of \c{id} propagates to the caller,
as can be seen by inspecting \c{pisum} with \c{code_warntype}.
%
Such type imprecision can impact performance in two ways. First,
the \c{sum} variable has to be boxed, adding a level of indirection to
any operation performed therein. Second, it is harder for
the compiler to devirtualize and inline consecutive calls, thus requiring
dynamic dispatch.

\subsection{Type Stability}\label{ssect:ts-informal}

Julia's compilation model is designed to accommodate source programs
with flexible types, yet to make such programs efficient. The compiler, by
default, creates an \emph{instance} of each source method for each distinct tuple of
argument types. Thus, even if the programmer does not provide any type
annotations, like in the \c{id} example, the compiler will create method
instances for \emph{concrete} input types seen during
an execution. For example, since in \c{pisum}, function \c{id} is called both
with a \c{Float64} and an \c{Int64}, the method table will hold two method instances
in addition to the original, user-defined method.
Because method instances have more precise argument types, the compiler can
leverage them to produce more efficient code and infer more precise return types.

In Julia parlance, a method is called \emph{type stable} if its inferred return
type depends solely on the types of its arguments. In the example, \c{id} is not
type stable, as its return type may change depending on the input \emph{value}
(in principle). On the contrary, the traditional implementation of the \c{id}
function is type stable: its return type is always the same as the type of its
sole input and does not depend on the input value, so, given the input type, the
return type is deducible.
The definition of type stability, though, is somewhat slippery.
The community has multiple, subtly different, informal definitions that capture
the same broad idea, but describe varying properties. The canonical definition
from the Julia documentation describes type stability as

\begin{itquote}
  ``[...] the type of the output is predictable from the types of the
  inputs. In particular, it means that the type of the output cannot vary
  depending on the values of the inputs.''
\end{itquote}
However, elsewhere, the documentation also states that \emph{``An analogous
type-stability problem exists for variables used repeatedly within a
function:''}
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,language=julia]
function foo()
    x = 1
    for i = 1:10
        x /= rand()
    end
    x
end
\end{lstlisting}
This function will always return a \c{Float64}, which is the type of \c{x}
at the end of the \c{foo} definition, regardless of the (nonexistent)
inputs. However, the manual says that it is a type stability issue nonetheless.
This is because the variable \c{x} is initialized to an \c{Int64} but then
assigned a \c{Float64} in the loop. Some versions of the compiler boxed \c{x} as
it could hold two different types; of course, in this example, one level of loop
unrolling would alleviate the performance issue, but in general,
imprecise types limit compiler optimizations.
Conveniently, the \c{code_warntype} macro mentioned above
will highlight imprecise types for \emph{all} intermediate variables.
Furthermore, the documentation states that
\begin{itquote}
  ``[t]his serves as a warning of potential type instability.''
\end{itquote}

Effectively, there are two competing, type-related properties
of function bodies. To address this confusion, we
define and refer to them using two distinct terms:
\begin{itemize}
  \item \emph{type stability} is when a function's return type depends only on
    its argument types, and
  \item \emph{type groundedness} is when every variable's type depends
    only on the argument types.
\end{itemize}
\MODIFY{Although type stability is strictly weaker than type groundedness,
for the purposes of this paper, we are interested in both properties.
The latter, type groundedness, is useful for performance of the function itself,
as it implies that unboxing, devirtualization, and inlining can occur.
The former, type stability, allows the function to be used efficiently
by other functions: namely, type-grounded functions may call a function that
is only type stable but not grounded.
For brevity, when the context is clear, we will refer to type stability and
type groundedness as stability and groundedness in what follows.
}

\subsection{Flavors of Type Stability}

\begin{figure}[t]\footnotesize
\begin{minipage}{4cm}\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,language=julia]
function f0(x)
  if (good(x))
    0
  else
    "not 0"
  end
end

function f1(x)
  if (good(x))
    0
  else
    1
  end
end
\end{lstlisting}\end{minipage}
\hspace{1cm}
\begin{minipage}{5cm}\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,language=julia]
function f2(x)
  if (good(x))
    x
  else
    -x
  end
end

function -(x::Float64)
  if (x==0)
    0
  else
    Base.neg_float(x)
  end
end
\end{lstlisting}\end{minipage}
\caption{\MODIFY{Flavors of stability: \c{f0} is unstable,
\c{f1} is stable; \c{f2} is stable if \c{(-)} is stable; \c{(-)} is unstable.}}\label{fla}
\Description{Several simple variants of a type-(un)stable function.}
\end{figure}

Type stability is an inter-procedural property, and in the worst case, it can depend
on the whole program. Consider the functions of Fig.~\ref{fla}. Function \c{f0}
is trivially type unstable, regardless of the type of its input: if \c{good(x)}
returns true, \c{f0} returns an \c{Int64} value, otherwise \c{f0} returns a
\c{String}. Function \c{f1} is trivially type stable as all control-flow paths
return a constant of \c{Int64} type. Function \c{f2} is type stable as
long as the negation operator is type stable and returns a value of the same type
as its argument. As an example, we show a method \c{-(::Float64)} that causes
\c{f2(::Float64)} to lose type stability. This is a common bug where the function
\c{(-)} either returns a \c{Float64} or \c{Int64} due to the constant \c{0} being
of type \c{Int64}.  The proper, Julia-style implementation for this method is
to replace the constant \c{0} with \c{zero(x)}, which returns the zero value
for the type of \c{x}, in this case \texttt{\small0\!.\!0}.
The example of function \c{f2} illustrates the fact that stability is a
whole-program property. Adding a method may cause some, seemingly unrelated,
method to lose type stability.

\subsection{Type Stability Versus Type Groundedness}

Type stability of a method is important for the groundedness of its callers.
Consider the function \c{h(x::Int64)=g(x)+1}. If \c{g(x)=x}, it follows that
\c{h} is both stable and grounded, as \c{g} will always return an \c{Int64}, and
so will \c{h}. However, if we define \c{g(x) = (x==2) ? "Ho" : 4}, then \c{h}
suddenly loses both properties. To recover stability and groundedness of \c{h},
it is necessary to make \c{g} type stable, yet it does not have to be grounded.
For example, despite the presence of the imprecise variable \c{y}, the following
definition makes \c{h} grounded: \c{g(x) = let y = (x==2 ? "Ho" : 4) in x
end}.

In practice, type stability is sought after when making architectural decisions.
Idiomatic Julia functions are small and have no internal type-directed
branching; instead, branches on types are replaced with multiple dispatch.
Once type ambiguity is lifted into dispatch, small functions with specific
type arguments are relatively easy to make type stable. In turn, this
architecture allows for effective devirtualization in a caller, as in many cases, the
inferred type at a call site will determine its callee at compilation time.

Thus, writing type-stable functions is a good practice, for it provides
callers of those functions with an opportunity to be efficient.
However, stability of callees is not a sufficient condition for the efficiency
of their callers: the callers themselves need to strive for type groundedness,
which requires eliminating type imprecision from control flow.

\subsection{Patterns of Instability}

There are several code patterns
that are inherently type unstable. For one, accessing abstract fields of
a structure is an unstable operation:
the concrete type of the field value depends on the struct value,
not just struct type. In Julia, it is recommended to avoid abstractly typed
fields if performance is important, %but there is nothing wrong with them
%from a software engineering point of view. For instance,
%abstract fields come in handy when interacting with external data sources.
but they are a useful tool for interacting with external data sources
and representing heterogenous data.

Another example is sum types (algebraic data types or open unions), which can be
modeled with subtyping in Julia. Take a hierarchy of an abstract type \c{Expr}
and its two concrete subtypes, \c{Lit} and \c{BinOp}.
Such a hierarchy is convenient, because
it allows for an \c{Expr} evaluator written with multiple dispatch:
\begin{lstlisting}[basicstyle=\footnotesize\ttfamily,language=julia]
run(e :: Lit) = ...                       run(e :: BinOp) = ...
\end{lstlisting}
If we want the evaluator to be called with the result of a function
\c{parse(s :: String)}, the latter cannot be type stable: \c{parse} will return
values of different concrete types, \c{Lit} and \c{BinOp}, depending on the
input string.
If one does want \c{parse} to be stable, they need to always
return the same concrete type, e.g. an S-expression-style struct. Then, \c{run}
has to be written without multiple dispatch, as a big if-expression, which may
be undesirable.
