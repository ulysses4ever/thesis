% ****************************************************************************************
%              Chap. 1: Intro
% ****************************************************************************************

\chapter{Introduction}%
\label{chap-intro}

\section{Julia Background}%
\label{sec-julia}

The Julia language is designed around multiple dispatch~\cite{BezansonEKS17}.
Programs consist of \emph{functions} that are implemented by multiple
\emph{methods} of the same name; each method is distinguished by a distinct type
signature, and all methods are stored in a so-called method table.
At run time, the Julia implementation dispatches a function call to
the \emph{most specific} method by comparing the types of the arguments to the
types of the parameters of all methods of that function. As an example of a
function and its constituent methods, consider the \c{+} function. As of version 1.5.4 of the
language, there are 190 implementations of \c{+}, each covering a
specific case determined by its type signature. Fig.~\ref{plus} displays custom
implementations for 16-bit floating point numbers, missing values,
big-floats/big-integers, and complex arithmetic.
Although at the source-code level, multiple methods look similar to
overloading known from languages like C++ and Java, the key difference is that
those languages resolve overloading statically whereas Julia does that
dynamically using multiple dispatch.

\begin{figure}
\begin{lstlisting}[language=julia]
# 184 methods for generic function "+":
[1] +(a::Float16, b::Float16) in Base at float.jl:398
[2] +(::Missing, ::Missing) in Base at missing.jl:114
[3] +(::Missing) in Base at missing.jl:100
[4] +(::Missing, ::Number) in Base at missing.jl:115
[5] +(a::BigFloat, b::BigFloat, c::BigFloat, d::BigFloat) in Base.MPFR at mpfr.jl:541
[6] +(a::BigFloat, b::BigFloat, c::BigFloat) in Base.MPFR at mpfr.jl:535
[7] +(x::BigFloat, c::BigInt) in Base.MPFR at mpfr.jl:394
[8] +(x::BigFloat, y::BigFloat) in Base.MPFR at mpfr.jl:363
...
\end{lstlisting}
\caption{Methods from the standard library}\label{plus}
\end{figure}

Julia supports a rich type language for defining method
signatures. Base types consist of either bits types---types that have a direct
binary representation, like integers---or structs. Both bits types and struct
types, referred to as \emph{concrete types}, can have supertypes, but all
supertypes are \emph{abstract types}. Abstract types can be arranged into a
single-subtyping hierarchy rooted at \c{Any}, and no abstract type can be
instantiated. The type language allows for further composition of these base
types using unions, tuples, and bounded existential constructors; the result of
composition can be abstract or concrete. \citet{oopsla18b} gives a detailed
discussion of the type language and of subtyping.

Any function call in a program, such as \c{x+y}, requires choosing one of the
methods of the target function. \emph{Method dispatch} chooses the method using
a multi-step process.
First, the implementation obtains the concrete types of arguments. Second, it
retrieves applicable methods by checking for subtyping between argument types
and type annotations of the methods. Next, it sorts these methods into subtype
order. Finally, the call is dispatched to the most specific method---a method
such that no other applicable method is its strict subtype. If no such
method exists, an error is produced. As an example, consider the above
definition of \c{+}: a call with two \c{BigFloat}'s dispatches to
definition 8 from \figref{plus}: \c{+(x::BigFloat, y::BigFloat)}.

Function calls are pervasive in Julia, and their efficiency is crucial for
performance. However, the many complex type-level operations involved in dispatch
make the process slow. Moreover, the language implementation, as of this writing,
does not perform inline caching~\cite{DS84}, meaning that dispatch results are
not cached across calls. To attain acceptable performance, the compiler attempts
to remove as many dispatch operations as it can. This optimization leverages
run-time type information whenever a method is compiled, i.e., when it is called
for the first time with a novel set of argument types.  These types are used by
the compiler to infer types in the method body. Then, this type information
frequently allows the compiler to devirtualize and inline the function calls
within a method~\cite{aigner}, thus improving performance. However, this
optimization is not always possible: if type inference cannot produce a
sufficiently specific type, then the call cannot be devirtualized. Consider the
prior example of \c{x+y}: the method to call cannot be determined
if \c{y} is known to be one of \c{BigFloat} or \c{BigInt}. This problem arises for
various reasons, for example, accessing a struct field of an abstract type, or
the type inferencer losing precision due to a branching statement. A more
detailed description of the compilation strategy and its performance is given in
\cite{oopsla18a}.

\section{Type Stability: a Key to Performance?}%
\label{sec:stability}

Removing dispatch is the key to performance, but to perform the optimization,
the compiler needs precise type information. Thus, while developers
are encouraged to write generic code, the code also needs to be conducive
to type inference and type-based optimizations.
% In this section,
% we give an overview of the appropriate coding discipline, and explain
% how it enables optimizations.

To illustrate performance implications of careless
coding practices, consider Fig.~\ref{ide}, which displays a method for one of
the Julia microbenchmarks, \c{pisum}. For the purposes of this example, we
have added an identity function \c{id} which was initially implemented to
return its argument in both branches, as well-behaved identities do.
Then, the \c{id} method was changed to return a string in the impossible
branch (\c{rand()} returns a value from 0 to 1). The impact of that change
was about a 40\% increase in the execution time of the benchmark (\juliaversion).

\lstdefinestyle{jterm}{
    basicstyle=\scriptsize\ttfamily,
    moredelim=[is][\bfseries\color{Red}]{a@}{a@},
    moredelim=[is][\color{MidnightBlue}]{b@}{b@},
    moredelim=[is][\bfseries\color{OliveGreen}]{`}{`},
}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{0.49\textwidth}
\centering
\begin{lstlisting}[language=julia,style=jterm]
function id(x)
  (rand() == 4.2) ? "x" : x
end


function pisum()
  sum = 0.0
  for j = 1:500
    sum = 0.0
    for k = 1:10000
      sum += id(1/(k*id(k)))
    end
  end
  sum
end
\end{lstlisting}
\caption{Microbenchmark, redacted}
\end{subfigure}
%
% \hspace{2mm}
\begin{subfigure}[b]{0.498\textwidth}
\begin{lstlisting}[style=jterm]
`julia>` @code_warntype id(5)
Variables
  #self#b@::Core.Compiler.Const(id, false)b@
  xb@::Int64b@
Bodya@::Union{Int64, String}a@
1 - %1 = Main.rand()b@::Float64b@
|   %2 = (%1 == 4.2)b@::Boolb@
+--      goto #3 if not %2
2 -      return "x"
3 -      return x

`julia>` @code_warntype pisum()
...
|   %20 = kb@::Int64b@
|   %21 = Main.id(k)a@::Union{Int64, String}a@
\end{lstlisting}
\caption{Julia session}
%
\end{subfigure}
\caption{A Julia microbenchmark (a) illustrating performance implications
  of careless coding practices: changing \c{id} function to return
  values of different types leads to longer execution
  because of the \c{Union} type of \c{id(..)}, which propagates to \c{pisum} (b).}%
\label{ide}
\end{figure}

When a performance regression occurs, it is common for developers to study the
intermediate representation produced by the compiler. To facilitate this, the
language provides a macro, \c{code_warntype}, that shows the code along with the
inferred types for a given function invocation. Fig.~\ref{ide} demonstrates the result of
calling \c{code_warntype} on \c{id(5)}. Types that are imprecise, i.e., not
concrete, show up in red: they indicate that concrete type of a value may vary
from run to run. Here, we see that when called with an integer,
\c{id} may return either an
\c{Int64} or a \c{String}.
Moreover, the imprecise return type of \c{id} propagates to the caller,
as can be seen by inspecting \c{pisum} with \c{code_warntype}.
%
Such type imprecision can impact performance in two ways. First,
the \c{sum} variable has to be boxed, adding a level of indirection to
any operation performed therein. Second, it is harder for
the compiler to devirtualize and inline consecutive calls, thus requiring
dynamic dispatch.

\paragraph{Type Stability}
Julia's compilation model is designed to accommodate source programs
with flexible types, yet to make such programs efficient. The compiler, by
default, creates an \emph{instance} of each source method for each distinct tuple of
argument types. Thus, even if the programmer does not provide any type
annotations, like in the \c{id} example, the compiler will create method
instances for \emph{concrete} input types seen during
an execution. For example, since in \c{pisum}, function \c{id} is called both
with a \c{Float64} and an \c{Int64}, the method table will hold two method instances
in addition to the original, user-defined method.
Because method instances have more precise argument types, the compiler can
leverage them to produce more efficient code and infer more precise return types.

In Julia parlance, a method is called \emph{type stable} if its inferred
return type depends solely on the types of its arguments; in the
example, \c{id} is not type stable, as its return type may change depending on
the input value (in principle). The traditional implementation of
the \c{id} function is type stable.

\section{Research Problem}
\label{chap-problem}

Optimizing dynamic languages is difficult, and remains an active research field.
Several state of the art JIT compilers do a good job but at the expense of being
opaque and unpredictable for the user.
Julia's unique design seemingly proposes a better approach based on the notion
of type stability. My thesis is, therefore:

\begin{itquote}
Type stability
is a widely used program property that can be leveraged by a compiler to generate
correct and efficient code
and can be approximated by automated techniques.
\end{itquote}

To validate this thesis I am going to work on the following tasks:
\begin{enumerate}

  \item
  Assess how widely type-stable code is deployed inside Julia's ecosystem, and
  whether any code patterns are associated with stable code.

  \item
  Establish a formal correspondence between type stability and code optimizations and
  show that optimized code is functionally equivalent to the initial version.

  \item
  Design an approach for approximating type stability without running the code and
  build a tool that shows that this approximation is viable in practice.
\end{enumerate}

% maybe limitations?
